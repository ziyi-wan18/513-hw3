{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## TECHIN 513 - Basic ML\n",
        "\n",
        "**Instructions**\n",
        "\n",
        "Install the required packages (scikit-learn, TensorFlow, Keras, PyTorch, and, pandas) if they are not already installed."
      ],
      "metadata": {
        "id": "JLRJNDGiPU37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use pip to install the packages\n",
        "!pip install scikit-learn TensorFlow Keras PyTorch pandas numpy\n",
        "!pip3 install torch torchvision torchaudio\n",
        "# Import necessary packages\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "# import RandomForestClassifier from sklearn\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n"
      ],
      "metadata": {
        "id": "JVHNfDPhPjyw",
        "outputId": "08bd7c50-033b-4e06-d4d2-866165f617be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: TensorFlow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting PyTorch\n",
            "  Using cached pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (4.9.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (1.60.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (2.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->TensorFlow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->TensorFlow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->TensorFlow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->TensorFlow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->TensorFlow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->TensorFlow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->TensorFlow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->TensorFlow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->TensorFlow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->TensorFlow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->TensorFlow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->TensorFlow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->TensorFlow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->TensorFlow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->TensorFlow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->TensorFlow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->TensorFlow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->TensorFlow) (3.2.2)\n",
            "Building wheels for collected packages: PyTorch\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for PyTorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for PyTorch\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for PyTorch\n",
            "Failed to build PyTorch\n",
            "\u001b[31mERROR: Could not build wheels for PyTorch, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n"
      ],
      "metadata": {
        "id": "VgftVsfsFd2i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Split the data into training and testing sets\n",
        "# use train_test_split function to split the data with test_size = 0.2 and random_state = 4\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Iris dataset (assuming this has been done previously)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n"
      ],
      "metadata": {
        "id": "ZdpM5-_DUYHf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Task 3: Train a Random Forest Classifier on the training data\n",
        "# import RandomForestClassifier from sklearn and fit it with training data\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Assuming you have already split the Iris dataset into training and testing sets\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "rf_clf = RandomForestClassifier(random_state=4)\n",
        "\n",
        "# Fit the model on the training data\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Now rf_clf is a trained Random Forest Classifier on your training data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "3xhhgROaUdP2",
        "outputId": "3fa51f3e-1cdb-485a-fd3a-d659b77ee5a1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(random_state=4)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=4)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Evaluate the classifier on the testing data\n",
        "# use clf.score function to evaluate the classifier on the testing data\n",
        "# print the accuracy of the classifier\n",
        "# Evaluate the classifier on the testing data\n",
        "accuracy = rf_clf.score(X_test, y_test)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"The accuracy of the Random Forest classifier on the testing data: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2aC43ejVIhB",
        "outputId": "9217ddb3-162d-4972-868f-ee049f4e61e8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of the Random Forest classifier on the testing data: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Load the MNIST dataset\n",
        "# use keras.datasets.mnist.load_data() to load the dataset\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Now X_train and X_test contain the images, and y_train and y_test contain the labels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11J2xUqFVKwy",
        "outputId": "c6494eed-b48c-49ad-f38f-d42f862bc581"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Preprocess the data\n",
        "# normalize the data by dividing by 255.0\n",
        "# use to_categorical from keras.utils to one-hot encode the labels\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Normalize the image data to values between 0 and 1\n",
        "X_train_normalized = X_train.astype('float32') / 255.0\n",
        "X_test_normalized = X_test.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train_one_hot = to_categorical(y_train)\n",
        "y_test_one_hot = to_categorical(y_test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sTMNWbudVX4I"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Task 7: Define and train a simple neural network using Keras\n",
        "# use Sequential model from keras.models\n",
        "# use Dense layer from keras.layers\n",
        "# use 'adam' as optimizer and 'categorical_crossentropy' as loss function\n",
        "# use model.fit to train the model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten\n",
        "\n",
        "# Define a simple neural network model\n",
        "model = Sequential([\n",
        "    # Flatten the input layer from 28x28 pixels to 784 pixels (28*28)\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    # Add a Dense hidden layer with 512 neurons and ReLU activation\n",
        "    Dense(512, activation='relu'),\n",
        "    # Add a Dense output layer with 10 neurons (one for each class) and softmax activation\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_normalized, y_train_one_hot,\n",
        "                    epochs=10,  # Number of epochs can be adjusted\n",
        "                    batch_size=128,  # Batch size can be adjusted\n",
        "                    validation_split=0.2,  # Use part of the training data for validation\n",
        "                    verbose=2)  # Verbose mode for logging the training progress\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPLtYbc-VZwr",
        "outputId": "ea2b7f6a-c440-4666-87ba-3460dc03a56f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "375/375 - 9s - loss: 0.2980 - accuracy: 0.9154 - val_loss: 0.1537 - val_accuracy: 0.9567 - 9s/epoch - 25ms/step\n",
            "Epoch 2/10\n",
            "375/375 - 4s - loss: 0.1219 - accuracy: 0.9645 - val_loss: 0.1083 - val_accuracy: 0.9688 - 4s/epoch - 10ms/step\n",
            "Epoch 3/10\n",
            "375/375 - 4s - loss: 0.0807 - accuracy: 0.9766 - val_loss: 0.0949 - val_accuracy: 0.9710 - 4s/epoch - 9ms/step\n",
            "Epoch 4/10\n",
            "375/375 - 4s - loss: 0.0571 - accuracy: 0.9834 - val_loss: 0.0882 - val_accuracy: 0.9747 - 4s/epoch - 12ms/step\n",
            "Epoch 5/10\n",
            "375/375 - 4s - loss: 0.0424 - accuracy: 0.9876 - val_loss: 0.0780 - val_accuracy: 0.9757 - 4s/epoch - 11ms/step\n",
            "Epoch 6/10\n",
            "375/375 - 4s - loss: 0.0306 - accuracy: 0.9913 - val_loss: 0.0819 - val_accuracy: 0.9745 - 4s/epoch - 10ms/step\n",
            "Epoch 7/10\n",
            "375/375 - 5s - loss: 0.0233 - accuracy: 0.9940 - val_loss: 0.0780 - val_accuracy: 0.9778 - 5s/epoch - 13ms/step\n",
            "Epoch 8/10\n",
            "375/375 - 4s - loss: 0.0175 - accuracy: 0.9959 - val_loss: 0.0735 - val_accuracy: 0.9786 - 4s/epoch - 10ms/step\n",
            "Epoch 9/10\n",
            "375/375 - 4s - loss: 0.0138 - accuracy: 0.9969 - val_loss: 0.0788 - val_accuracy: 0.9785 - 4s/epoch - 10ms/step\n",
            "Epoch 10/10\n",
            "375/375 - 5s - loss: 0.0097 - accuracy: 0.9982 - val_loss: 0.0706 - val_accuracy: 0.9803 - 5s/epoch - 12ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Evaluate the neural network on the testing data\n",
        "# use model.evaluate to get the test loss and test accuracy\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = model.evaluate(X_test_normalized, y_test_one_hot, verbose=2)\n",
        "\n",
        "# Print the test loss and test accuracy\n",
        "print(f\"Test loss: {test_loss:.4f}\")\n",
        "print(f\"Test accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI6MGlHcVbdj",
        "outputId": "4562fc82-74e4-44f2-8ae5-529ac95e4380"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - loss: 0.0608 - accuracy: 0.9811 - 1s/epoch - 4ms/step\n",
            "Test loss: 0.0608\n",
            "Test accuracy: 0.9811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Define a simple linear regression model using PyTorch\n",
        "# create a class LinearRegression that inherit from nn.Module\n",
        "# define the constructor and forward function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the Linear Regression model class\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LinearRegression, self).__init__()\n",
        "        # Define the single linear layer\n",
        "        self.linear = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the forward pass through the linear layer\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "# Assuming we're working with a single feature and a single target value\n",
        "input_size = 1\n",
        "output_size = 1\n",
        "\n",
        "# Create an instance of the LinearRegression model\n",
        "model = LinearRegression(input_size, output_size)\n"
      ],
      "metadata": {
        "id": "mUynu_Y3Vddq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Task 10: Train the linear regression model on some dummy data and print the weight and bias\n",
        "# create an instance of LinearRegression\n",
        "# use nn.MSELoss as criterion, optim.SGD as optimizer\n",
        "# use model.parameters() as input for optimizer\n",
        "# use optimizer.step() and criterion to update the model weight and bias\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Assuming the LinearRegression class has been defined as previously shown\n",
        "\n",
        "# Create some dummy data\n",
        "x_train = torch.randn(100, 1) * 10  # 100 data points\n",
        "y_train = x_train + 3 * torch.randn(100, 1)  # y = x + some noise\n",
        "\n",
        "# Instantiate the model\n",
        "input_size = 1\n",
        "output_size = 1\n",
        "model = LinearRegression(input_size, output_size)\n",
        "\n",
        "# Define the loss criterion and the optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Learning rate can be adjusted\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass to get the output\n",
        "    predictions = model(x_train)\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = criterion(predictions, y_train)\n",
        "\n",
        "    # Backward pass to get the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print loss every 10 epochs\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Print the model's weight and bias after training\n",
        "weight, bias = model.linear.weight.item(), model.linear.bias.item()\n",
        "print(f\"Weight: {weight:.4f}\")\n",
        "print(f\"Bias: {bias:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sQFUhFDVf9R",
        "outputId": "04892754-a719-4a10-9782-837a1db68e59"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 175259.4531\n",
            "Epoch [20/100], Loss: 163093184.0000\n",
            "Epoch [30/100], Loss: 151780884480.0000\n",
            "Epoch [40/100], Loss: 141253370970112.0000\n",
            "Epoch [50/100], Loss: 131455342474166272.0000\n",
            "Epoch [60/100], Loss: 122337574680868683776.0000\n",
            "Epoch [70/100], Loss: 113852268595021057359872.0000\n",
            "Epoch [80/100], Loss: 105955257160800313211355136.0000\n",
            "Epoch [90/100], Loss: 98605977808469434199166681088.0000\n",
            "Epoch [100/100], Loss: 91766454445441730284225174700032.0000\n",
            "Weight: -1228850272927744.0000\n",
            "Bias: -10759317749760.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus"
      ],
      "metadata": {
        "id": "goIaALYXVy1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task: Implement a Convolutional Neural Network to classify the CIFAR-10 dataset\n",
        "# use torchvision.datasets.CIFAR10 to load the dataset\n",
        "# create a class CNN that inherit from nn.Module\n",
        "# define the constructor, forward function and the network architecture\n",
        "# use CrossEntropyLoss as criterion, optim.SGD as optimizer\n",
        "# use model.parameters() as input for optimizer\n",
        "# use optimizer.step() and criterion to update the model weight and bias\n",
        "model = CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "id": "-ZYu5X7gV1L9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5000f7c1-fc24-4442-a882-85d02d28b56b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 2.240\n",
            "[1,  4000] loss: 1.995\n",
            "[1,  6000] loss: 1.736\n",
            "[1,  8000] loss: 1.645\n",
            "[1, 10000] loss: 1.564\n",
            "[1, 12000] loss: 1.513\n",
            "[2,  2000] loss: 1.420\n",
            "[2,  4000] loss: 1.405\n",
            "[2,  6000] loss: 1.371\n",
            "[2,  8000] loss: 1.334\n",
            "[2, 10000] loss: 1.305\n",
            "[2, 12000] loss: 1.297\n",
            "[3,  2000] loss: 1.231\n",
            "[3,  4000] loss: 1.241\n",
            "[3,  6000] loss: 1.232\n",
            "[3,  8000] loss: 1.209\n",
            "[3, 10000] loss: 1.189\n",
            "[3, 12000] loss: 1.176\n",
            "[4,  2000] loss: 1.135\n",
            "[4,  4000] loss: 1.129\n",
            "[4,  6000] loss: 1.145\n",
            "[4,  8000] loss: 1.121\n",
            "[4, 10000] loss: 1.121\n",
            "[4, 12000] loss: 1.118\n",
            "[5,  2000] loss: 1.051\n",
            "[5,  4000] loss: 1.050\n",
            "[5,  6000] loss: 1.045\n",
            "[5,  8000] loss: 1.074\n",
            "[5, 10000] loss: 1.067\n",
            "[5, 12000] loss: 1.060\n",
            "[6,  2000] loss: 0.963\n",
            "[6,  4000] loss: 1.001\n",
            "[6,  6000] loss: 1.019\n",
            "[6,  8000] loss: 1.017\n",
            "[6, 10000] loss: 1.010\n",
            "[6, 12000] loss: 1.025\n",
            "[7,  2000] loss: 0.923\n",
            "[7,  4000] loss: 0.961\n",
            "[7,  6000] loss: 0.958\n",
            "[7,  8000] loss: 0.949\n",
            "[7, 10000] loss: 0.984\n",
            "[7, 12000] loss: 0.990\n",
            "[8,  2000] loss: 0.869\n",
            "[8,  4000] loss: 0.923\n",
            "[8,  6000] loss: 0.919\n",
            "[8,  8000] loss: 0.944\n",
            "[8, 10000] loss: 0.927\n",
            "[8, 12000] loss: 0.951\n",
            "[9,  2000] loss: 0.855\n",
            "[9,  4000] loss: 0.869\n",
            "[9,  6000] loss: 0.881\n",
            "[9,  8000] loss: 0.907\n",
            "[9, 10000] loss: 0.916\n",
            "[9, 12000] loss: 0.905\n",
            "[10,  2000] loss: 0.810\n",
            "[10,  4000] loss: 0.835\n",
            "[10,  6000] loss: 0.846\n",
            "[10,  8000] loss: 0.865\n",
            "[10, 10000] loss: 0.881\n",
            "[10, 12000] loss: 0.902\n",
            "Finished Training\n"
          ]
        }
      ]
    }
  ]
}